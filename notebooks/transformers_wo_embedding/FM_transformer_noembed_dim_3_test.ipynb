{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92608fc3-14be-4de6-99c3-0bec3b1a8f5c",
   "metadata": {},
   "source": [
    "### Different params for transformer encoder. Increase # of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e1ac83-c0fc-4879-bfe3-c710ce17ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import math\n",
    "\n",
    "from scipy.linalg import expm, hadamard, signm\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb49572a-0f65-49c8-b78d-d7e77a717841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/rahulpadmanabhan/Development/ws1/masters_thesis_2/LAWT/src/neuralnet')\n",
    "# sys.path.insert(0, '/home/rahulpadmanabhan/Development/ws1/masters_thesis_2/LAWT/src/')\n",
    "import datagenerator\n",
    "from models import MatrixFunctionTransformer\n",
    "from common import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc6646f-fa77-45ff-b7f3-713f2d78116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc89b32-ecf7-4a2b-895c-14bda8b1e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad42912-31d6-4eb2-9d28-14876a6f506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3af2d614-aa01-42e7-9200-ffe7d9ff0ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:26:18,510 - root - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaca555-663b-416d-9591-0822607c8f4d",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7add3eb4-afff-4899-99fd-613b0849c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "operation = \"sign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dee6981e-0423-4a2f-9983-46dec48f4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = 2**15  # 16384\n",
    "k_values = range(5, 19)\n",
    "train_samples = [32768]\n",
    "dim = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "800a058e-dc61-4fc2-83fe-10066601008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:30:21,036 - root - INFO - train_samples=[32768], test_samples=32768\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"{train_samples=}, {test_samples=}\")\n",
    "\n",
    "relative_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f07215c-f4d1-4e73-9a67-44e421e4a8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:30:21,533 - root - INFO - models_lst=['/mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843/dim_3/sign_model_16384.pth']\n",
      "\n",
      "train_lst=['/mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843/dim_3/train/train_dataset_16384.pt']\n",
      "\n",
      "test_lst=['/mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843/dim_3/test/test_dataset_16384.pt']\n"
     ]
    }
   ],
   "source": [
    "train_vals = [32768 - 16384]\n",
    "models_lst = [*map(lambda x: os.path.join(save_dir, f\"dim_{dim}\", f'{operation}_model_{str(x)}.pth'), train_vals)]\n",
    "train_lst = [*map(lambda x: os.path.join(save_dir, f\"dim_{dim}\", \"train\", f'train_dataset_{str(x)}.pt'), train_vals)]\n",
    "test_lst = [*map(lambda x: os.path.join(save_dir, f\"dim_{dim}\", \"test\", f'test_dataset_{str(x)}.pt'), train_vals)]\n",
    "\n",
    "logger.info(f\"{models_lst=}\\n\\n{train_lst=}\\n\\n{test_lst=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3aa550b0-941a-495c-9af2-d8c4ed6597f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843/dim_3/sign_model_16384.pth'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a1d8027-f21c-4123-854e-14eee7c73fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:32:08,277 - root - INFO - MatrixFunctionTransformer(\n",
      "  (encoder1): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=3, out_features=12, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=12, out_features=3, bias=True)\n",
      "        (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder2): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=3, out_features=12, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=12, out_features=3, bias=True)\n",
      "        (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dim=3\n",
    "d_model = dim*dim\n",
    "# Ensure d_model is divisible by nhead\n",
    "nhead = dim\n",
    "d_model = (dim // nhead) * nhead\n",
    "# Load the saved model\n",
    "model = MatrixFunctionTransformer(d_model, dim, 10).to(device)\n",
    "model.load_state_dict(torch.load(models_lst[0]))\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "324558c8-0bd0-4ac2-acd6-9d92fb88b717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixFunctionTransformer(\n",
       "  (encoder1): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-9): 10 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=3, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=12, out_features=3, bias=True)\n",
       "        (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder2): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-9): 10 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=3, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=12, out_features=3, bias=True)\n",
       "        (norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models_lst[]\n",
    "dim=3\n",
    "d_model = dim*dim\n",
    "# Ensure d_model is divisible by nhead\n",
    "nhead = dim\n",
    "d_model = (dim // nhead) * nhead\n",
    "model = MatrixFunctionTransformer(d_model, dim, 10).to(device)\n",
    "model.load_state_dict(torch.load(models_lst[0]))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe8537e7-3081-4f53-908d-e533a97af984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 00:46:20,307 - root - INFO - idx=0\n",
      "Loaded model: /mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843/dim_3/sign_model_16384.pth\n",
      "2024-09-09 00:46:20,307 - root - INFO - len(test_dataset)=32768\n",
      "2024-09-09 00:46:20,311 - root - INFO - Starting to predict values in the test_dataset\n",
      "2024-09-09 00:46:20,311 - root - INFO - dim=3\n",
      "2024-09-09 00:49:53,552 - root - INFO - \n",
      "predicted.shape=torch.Size([32768, 3, 3])\n",
      "actuals.shape=torch.Size([32768, 3, 3])\n",
      "2024-09-09 00:49:53,553 - root - INFO - np_predicted.dtype=dtype('float64'),np_actuals.dtype=dtype('float64')\n",
      "2024-09-09 00:49:53,559 - root - INFO - mu=0.5598223915786457,sigma=1.8779931422997431\n",
      "2024-09-09 00:49:53,560 - root - INFO - y_main=3.6292960126993457,u_shaded=0.04806503349329885,l_shaded=274.04099384705\n",
      "2024-09-09 00:49:53,560 - root - INFO - x_raw.shape=(294912,), y_raw.shape=(294912,)\n"
     ]
    }
   ],
   "source": [
    "mu_lst = []\n",
    "sigma_lst = []\n",
    "y_main_lst = []\n",
    "u_shaded_lst = []\n",
    "l_shaded_lst = []\n",
    "dim=3\n",
    "for idx, train_val in enumerate(train_vals):\n",
    "\n",
    "    # Load the saved model\n",
    "    d_model = dim*dim\n",
    "    # Ensure d_model is divisible by nhead\n",
    "    nhead = dim\n",
    "    d_model = (dim // nhead) * nhead\n",
    "    model = MatrixFunctionTransformer(d_model, dim, 10).to(device)\n",
    "    model.load_state_dict(torch.load('/mnt/wd_2tb/thesis_transformers/experiments/transformer_wo_embedding/sign/encoder_20240908223843/dim_3/sign_model_16384.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    # Loading the test dataset\n",
    "    test_dataset = torch.load(test_lst[idx])\n",
    "\n",
    "    logger.info(f\"{idx=}\\nLoaded model: {models_lst[idx]}\")\n",
    "    logger.info(f\"{len(test_dataset)=}\")\n",
    "\n",
    "    actuals = []\n",
    "    predicted = []\n",
    "\n",
    "    logger.info(f\"Starting to predict values in the test_dataset\")\n",
    "    with torch.no_grad():\n",
    "        logger.info(f\"{dim=}\")\n",
    "        for x, y in test_dataset:\n",
    "            x = x.view(-1, dim, dim).to(device).to(torch.float64)\n",
    "            y = y.view(-1, dim, dim).to(device).to(torch.float64)\n",
    "            # if dim == 1:\n",
    "                # predicted.append(model(x.view(-1, 1).to(device).to(torch.float64)))\n",
    "                # actuals.append(y.view(-1,1).to(device).to(torch.float64))\n",
    "            # else:\n",
    "                # predicted.append(model(x.view(1, dim*dim).to(device).to(torch.float64)))\n",
    "                # actuals.append(y.view(1, dim*dim).to(device).to(torch.float64))\n",
    "                    \n",
    "            predicted.append(model(x))\n",
    "            actuals.append(y)\n",
    "            \n",
    "        predicted = torch.cat(predicted, 0)\n",
    "        actuals = torch.cat(actuals, 0)\n",
    "        \n",
    "        logger.info(f\"\\n{predicted.shape=}\\n{actuals.shape=}\")\n",
    "        \n",
    "        np_predicted = predicted.view(-1).cpu().numpy()\n",
    "        np_actuals = actuals.view(-1).cpu().numpy()\n",
    "\n",
    "        logger.info(f\"{np_predicted.dtype=},{np_actuals.dtype=}\")\n",
    "        \n",
    "        # mu = np.mean(np.log10(np_predicted))\n",
    "        y_is = np.abs(np_predicted - np_actuals)/(np.abs(np_actuals + 1e-6) ) # did this in a hurry, double check denominator\n",
    "        mu = np.mean(np.log10(y_is))\n",
    "        sigma = (np_predicted.shape[0] -1)**-1 * np.sum((np.log10(y_is) - mu)**2)\n",
    "        \n",
    "        y_main = 10**mu \n",
    "        \n",
    "        u_shaded = 10**(mu - sigma)\n",
    "        l_shaded = 10**(mu + sigma)\n",
    "        \n",
    "        logger.info(f\"{mu=},{sigma=}\")\n",
    "        logger.info(f\"{y_main=},{u_shaded=},{l_shaded=}\")\n",
    "        \n",
    "        x_raw = np.repeat(train_vals[idx], np_predicted.shape[0])\n",
    "        y_raw = np_actuals\n",
    "        \n",
    "        logger.info(f\"{x_raw.shape=}, {y_raw.shape=}\")\n",
    "\n",
    "        mu_lst.append(mu)\n",
    "        sigma_lst.append(sigma)\n",
    "        y_main_lst.append(y_main)\n",
    "        u_shaded_lst.append(u_shaded)\n",
    "        l_shaded_lst.append(l_shaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef6d5bf8-37e0-4928-a13f-ab5d68026da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5598223915786457]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_lst # Relative error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd139adf-f0f7-4882-a37f-d98ba0e719b7",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Increasing Epochs and heads of attention has a better relative error. Went to only ~55% compared to over 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
